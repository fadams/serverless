#!/bin/bash
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# The number of nodes in the Kubernetes cluster.
#NUM_NODES=${NUM_NODES:-4}
NUM_NODES=${NUM_NODES:-1}

# Set up kubeadm-dind-cluster to use local registry
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/119
#
# Note that there is an issue with insecure registries not working
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/266
# jc-sanchez created a patch for this here
# https://github.com/jc-sanchez/kubeadm-dind-cluster/commit/7db13f425a78c252c2b6b7fcc9866320a0fc79e1
# However that has not yet made it to the public images!
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/288
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/pull/301
#export DIND_INSECURE_REGISTRIES="[\"docker-mint.local:5000\"]"


COMMAND="${1:-}"

# Provision Kubernetes and Helm
#-------------------------------------------------------------------------------

# Deploy kubeadm-dind-cluster
function cluster::deploy-kubeadm-dind-cluster {
    echo "Starting $NUM_NODES node Kubernetes cluster"
    # Redirect startup logs to kubeadm-dind-cluster-log.txt so we can extract
    # the dashboard URL later.
    NUM_NODES=$NUM_NODES ./dind-cluster-v1.13.sh up 2>&1 | tee kubeadm-dind-cluster-log.txt

    # Grok dashboard URL from startup logs
    DASHBOARD_URL=$(cat kubeadm-dind-cluster-log.txt | sed -n -e 's/* Access dashboard at: //p')
    echo "DASHBOARD_URL: $DASHBOARD_URL"
    # Tidy up startup logs now that we've got the dashboard URL
    rm kubeadm-dind-cluster-log.txt

    # Open default browser at Kubernetes dashboard URL.
    xdg-open $DASHBOARD_URL
}

# Deploy Helm
function cluster::deploy-helm {
    if [ ! -d helm ]; then
        echo "Downloading Helm v2.13.0"
        curl -sSL https://storage.googleapis.com/kubernetes-helm/helm-v2.13.0-linux-amd64.tar.gz -o helm.tar.gz
        tar zxf helm.tar.gz && mv linux-amd64 helm && rm helm.tar.gz
        ln -s $PWD/helm/helm $HOME/bin/helm
    fi

    echo "Installing Tiller into Kubernetes cluster $(kubectl config current-context)"
    # Create RBAC permissions for Tiller
    # See https://github.com/openfaas/faas-netes/blob/master/HELM.md
    kubectl -n kube-system create sa tiller \
      && kubectl create clusterrolebinding tiller \
         --clusterrole cluster-admin \
         --serviceaccount=kube-system:tiller

    helm init --history-max 200 --skip-refresh --upgrade --service-account tiller

    # Wait for Tiller to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n kube-system | grep 'tiller-' | tail -n1 | grep -o 'Running')
        echo "Tiller state: $STATE"
        sleep 5
    done
}


# Provision OpenFaaS via Helm
#-------------------------------------------------------------------------------

# Deploy OpenFaaS via Helm
# https://github.com/openfaas/faas-netes/blob/master/chart/openfaas/README.md
function cluster::deploy-openfaas {
    echo "Starting OpenFaaS on Kubernetes cluster"

    # Create two namespaces for OpenFaaS core services and for the functions:
    kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml

    # Render the chart to a Kubernetes manifest called openfaas.yaml
    #helm template faas-netes/chart/openfaas \
    #--name openfaas \
    #--namespace openfaas  \
    #--set functionNamespace=openfaas-fn > openfaas.yaml

    # Edit
#    kubectl apply -f ./openfaas.yaml




    # Add the OpenFaaS helm chart:
#    helm repo add openfaas https://openfaas.github.io/faas-netes/

    # Deploy OpenFaaS from the helm chart repo. N.B. note that no authentication
    # has been applied. The link above illustrates enabling basic authentication
    # for the gateway but let's consider authentication more when the basics
    # are working properly.
#    helm repo update
    
#    helm upgrade openfaas --install openfaas/openfaas \
#        --namespace openfaas  \
#        --set functionNamespace=openfaas-fn


    # kubectl --namespace=openfaas port-forward svc/gateway 8080:8080

    # kubectl --namespace=openfaas get deployments -l "release=openfaas, app=openfaas"
    # helm delete --purge openfaas

    # Creates alertmanager, faas-idler, gateway, nats, prometheus, queue-worker
}

# Deploy OpenFaaS via kubectl and YAML (plan B if helm install fails)
function cluster::deploy-openfaas-via-yaml {
    echo "Starting OpenFaaS on Kubernetes cluster"

    kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml
    kubectl apply -f faas-netes/yaml
    # Creates alertmanager, gateway, nats, prometheus, queue-worker
}

# Run Grafana in OpenFaaS Kubernetes namespace and expose with NodePort.
# Instructions taken from OpnFaaS workshop lab 2.
# https://github.com/openfaas/workshop/blob/kubernetes-preview/lab2.md
function cluster::deploy-openfaas-grafana {
    # This uses a prebuilt image for this project:
    # https://github.com/stefanprodan/faas-grafana

    # Run Grafana in OpenFaaS Kubernetes namespace
    kubectl -n openfaas run \
        --image=stefanprodan/faas-grafana:4.6.3 \
        --port=3000 \
        grafana

    # Expose Grafana Service with a NodePort:
    kubectl -n openfaas expose deployment grafana \
        --type=NodePort \
        --name=grafana
}

# Run OpenFaaS UI
function cluster::run-openfaas-ui {
    # Wait for OpenFaaS Gateway to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'gateway-' | tail -n1 | grep -o 'Running')
        echo "OpenFaaS Gateway state: $STATE"
        sleep 5
    done

    # Open OpenFaaS Portal in new tab
    xdg-open http://10.192.0.2:31112

    # Wait for Prometheus to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'prometheus-' | tail -n1 | grep -o 'Running')
        echo "Prometheus state: $STATE"
        sleep 5
    done

    # Open OpenFaaS Prometheus metrics UI in new tab
    xdg-open http://10.192.0.2:31119

    # Wait for Grafana to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'grafana-' | tail -n1 | grep -o 'Running')
        echo "Grafana state: $STATE"
        sleep 5
    done

    GRAFANA_PORT=$(kubectl -n openfaas get svc grafana -o jsonpath="{.spec.ports[0].nodePort}")

    GRAFANA_URL=http://10.192.0.2:$GRAFANA_PORT/dashboard/db/openfaas

    echo "GRAFANA_URL=$GRAFANA_URL"

    # Open OpenFaaS Grafana metrics UI in new tab
    xdg-open $GRAFANA_URL 
}

# Deploy OpenFaaS Demo Functions, taken from:
# https://github.com/openfaas/workshop/blob/kubernetes-preview/lab2.md
# faas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml
function cluster::deploy-openfaas-demo-functions {
    OPENFAAS_URL=http://10.192.0.2:31112 faas-cli deploy -f ./openfaas-demo-functions.yaml
}


# Provision Fn
#-------------------------------------------------------------------------------

# Deploy Fn
function cluster::deploy-fn {
    # https://dzone.com/articles/serverless-with-fn-project-on-kubernetes-for-docke
    # https://github.com/fnproject/fn-helm
    # https://medium.com/@brianbmathews/going-serverless-on-oracle-cloud-using-the-open-source-fn-project-3c71f843b6d

    # TODO do this via YAML?
    kubectl create namespace fn

    git clone https://github.com/fnproject/fn-helm.git

    helm dep build fn-helm/fn


    # These two calls seem to be equivalent.
#    helm install --name fn --namespace fn fn-helm/fn
#    helm upgrade fn --install fn-helm/fn --namespace fn


    # Generate YAML from Fn Helm chart. We need to edit the YAML as the
    # available Helm configuration doesn't support our needs.
#    helm template --name fn --namespace fn fn-helm/fn > fn.yaml
    kubectl apply -f ./fn.yaml


    # To undeploy Fn from cluster.
    # helm delete --purge fn
    # kubectl delete namespace fn   
}

function cluster::up {
    cluster::deploy-kubeadm-dind-cluster

    cluster::deploy-helm

    #cluster::deploy-openfaas
    cluster::deploy-openfaas-via-yaml # Plan B

    cluster::deploy-openfaas-grafana

    cluster::run-openfaas-ui

    cluster::deploy-openfaas-demo-functions
}


#-------------------------------------------------------------------------------

case ${COMMAND} in
  up)
    cluster::up
    ;;
  down)
    echo "Stopping cluster"
    ./dind-cluster-v1.13.sh down
    ;;
  clean)
    echo "Removing cluster containers and volumes"
    ./dind-cluster-v1.13.sh clean
    ;;
  *)
    echo "usage:" >&2
    echo "  $0 up - startup the cluster and provision services" >&2
#    echo "  $0 reup" >&2
    echo "  $0 down - stop and tear down the cluster" >&2
#    echo "  $0 init kubeadm-args..." >&2
#    echo "  $0 join kubeadm-args..." >&2
    # echo "  $0 bare container_name [docker_options...]"
    echo "  $0 clean - remove cluster containers and volumes"
#    echo "  $0 copy-image [image_name]" >&2
#    echo "  $0 e2e [test-name-substring]" >&2
#    echo "  $0 e2e-serial [test-name-substring]" >&2
#    echo "  $0 dump" >&2
#    echo "  $0 dump64" >&2
#    echo "  $0 split-dump" >&2
#    echo "  $0 split-dump64" >&2
    exit 1
    ;;
esac

