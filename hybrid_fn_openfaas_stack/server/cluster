#!/bin/bash
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Prerequisites: Requires Docker to be installed as well as the kubectl, helm,
# OpenFaaS and Fn CLIs to be available on the user's PATH.
# TODO check for prerequisites and install if not present?

# Get the cluster command to run from stdin.
COMMAND="${1:-}"

# The number of nodes in the Kubernetes cluster.
NUM_NODES=${NUM_NODES:-4}

# Set up kubeadm-dind-cluster to use local registry at $(hostname).local:5000
# where $(hostname).local is a private address resolved by mDNS.
# https://en.wikipedia.org/wiki/.local
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/119
#
# Note that there is an issue with insecure registries not working
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/266
# jc-sanchez created a patch for this here
# https://github.com/jc-sanchez/kubeadm-dind-cluster/commit/7db13f425a78c252c2b6b7fcc9866320a0fc79e1
# However that has not yet made it to the public images!
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/issues/288
# https://github.com/kubernetes-sigs/kubeadm-dind-cluster/pull/301
#
# A work-around is to append "|| true" to the line restarting Docker. e.g.
# docker exec ${container_id} systemctl restart docker || true
# in dind::custom-docker-opts this forces the command to exit successfully.
export DIND_INSECURE_REGISTRIES="[\"$(hostname).local:5000\"]"






# Provision Kubernetes and Helm
#-------------------------------------------------------------------------------

# Deploy kubeadm-dind-cluster
function cluster::deploy-kubeadm-dind-cluster {
    echo "Starting $NUM_NODES node Kubernetes cluster"
    # Redirect startup logs to kubeadm-dind-cluster-log.txt so we can extract
    # the dashboard URL later.
    NUM_NODES=$NUM_NODES ./dind-cluster-v1.13.sh up 2>&1 | tee kubeadm-dind-cluster-log.txt

    # Grok dashboard URL from startup logs
    DASHBOARD_URL=$(cat kubeadm-dind-cluster-log.txt | sed -n -e 's/* Access dashboard at: //p')
    echo "DASHBOARD_URL: $DASHBOARD_URL"
    # Tidy up startup logs now that we've got the dashboard URL
    rm kubeadm-dind-cluster-log.txt

    # Open default browser at Kubernetes dashboard URL.
    xdg-open $DASHBOARD_URL
}

# Deploy Helm
function cluster::deploy-helm {
    if [ ! -d helm ]; then
        echo "Downloading Helm v2.13.0"
        curl -sSL https://storage.googleapis.com/kubernetes-helm/helm-v2.13.0-linux-amd64.tar.gz -o helm.tar.gz
        tar zxf helm.tar.gz && mv linux-amd64 helm && rm helm.tar.gz
        ln -s $PWD/helm/helm $HOME/bin/helm
    fi

    echo "Installing Tiller into Kubernetes cluster $(kubectl config current-context)"
    # Create RBAC permissions for Tiller
    # See https://github.com/openfaas/faas-netes/blob/master/HELM.md
    kubectl -n kube-system create sa tiller \
      && kubectl create clusterrolebinding tiller \
         --clusterrole cluster-admin \
         --serviceaccount=kube-system:tiller

    helm init --history-max 200 --skip-refresh --upgrade --service-account tiller

    # Wait for Tiller to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n kube-system | grep 'tiller-' | tail -n1 | grep -o 'Running')
        echo "Tiller state: $STATE"
        sleep 5
    done
}


# Provision OpenFaaS via Helm
#-------------------------------------------------------------------------------

# Deploy OpenFaaS via Helm
# This is currently commented out and the OpenFaaS YAML Kubernetes config is
# used because *.svc.cluster.local addresses seem to cause issues. This is DNS
# related and I *think* that the issue is because I'm using mDNS/AVAHI so that
# hosts on my local private network can be resolved using <hostname>.local
# addresses, but I think that the default ndots:0 in /etc/resolv.conf means that
# *.svc.cluster.local are being looked up and failing to be resolved. Using
# *.svc.cluster.local. (with a trailing period) specify a fully qualified name
# which fixes the issue but tweaking kebeadm-dind-cluster DNS config may be
# a better approach when I can figure out how to do that. Interestingly the
# OpenFaaS YAML in faas-netes/yaml actually uses period terminated fully
# qualified addresses whereas the Helm chart doesn't and I onle figured out
# this issue when I rendered the Helm chart to a Kubernetes manifest and
# compared that with the original YAML to see what was different between them.
#
# https://github.com/openfaas/faas-netes/blob/master/chart/openfaas/README.md
function cluster::deploy-openfaas {
    echo "Starting OpenFaaS on Kubernetes cluster"

    # Create two namespaces for OpenFaaS core services and for the functions:
    kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml

    # Render the chart to a Kubernetes manifest called openfaas.yaml
    #helm template faas-netes/chart/openfaas \
    #--name openfaas \
    #--namespace openfaas  \
    #--set functionNamespace=openfaas-fn > openfaas.yaml

    # Edit
#    kubectl apply -f ./openfaas.yaml




    # Add the OpenFaaS helm chart:
#    helm repo add openfaas https://openfaas.github.io/faas-netes/

    # Deploy OpenFaaS from the helm chart repo. N.B. note that no authentication
    # has been applied. The link above illustrates enabling basic authentication
    # for the gateway but let's consider authentication more when the basics
    # are working properly.
#    helm repo update
    
#    helm upgrade openfaas --install openfaas/openfaas \
#        --namespace openfaas  \
#        --set functionNamespace=openfaas-fn


    # kubectl --namespace=openfaas port-forward svc/gateway 8080:8080

    # kubectl --namespace=openfaas get deployments -l "release=openfaas, app=openfaas"
    # helm delete --purge openfaas

    # Creates alertmanager, faas-idler, gateway, nats, prometheus, queue-worker
}

# Deploy OpenFaaS via kubectl and YAML (plan B if helm install fails)
function cluster::deploy-openfaas-via-yaml {
    echo "Starting OpenFaaS on Kubernetes cluster"

    kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml
    kubectl apply -f faas-netes/yaml
    # Creates alertmanager, gateway, nats, prometheus, queue-worker
}

# Run Grafana in OpenFaaS Kubernetes namespace and expose with NodePort.
# Instructions taken from OpnFaaS workshop lab 2.
# https://github.com/openfaas/workshop/blob/kubernetes-preview/lab2.md
function cluster::deploy-openfaas-grafana {
    # This uses a prebuilt image for this project:
    # https://github.com/stefanprodan/faas-grafana

    # Run Grafana in OpenFaaS Kubernetes namespace
    kubectl -n openfaas run \
        --image=stefanprodan/faas-grafana:4.6.3 \
        --port=3000 \
        grafana

    # Expose Grafana Service with a NodePort:
    kubectl -n openfaas expose deployment grafana \
        --type=NodePort \
        --name=grafana
}

# Wait until the OpenFaaS services become available.
function cluster::wait-for-openfaas-services {
    # Wait for OpenFaaS Gateway to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'gateway-' | tail -n1 | grep -o 'Running')
        echo "OpenFaaS Gateway state: $STATE"
        sleep 5
    done

    # Wait for Prometheus to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'prometheus-' | tail -n1 | grep -o 'Running')
        echo "Prometheus state: $STATE"
        sleep 5
    done

    # Wait for Grafana to start
    STATE="unknown"
    while [ "$STATE" != "Running" ]; do
        STATE=$(kubectl get pod -n openfaas | grep 'grafana-' | tail -n1 | grep -o 'Running')
        echo "Grafana state: $STATE"
        sleep 5
    done
}

# Run OpenFaaS UI
# N.B. must call cluster::wait-for-openfaas-services first to ensure that the
# services are actually available.
function cluster::run-openfaas-ui {
    # Open OpenFaaS Portal in new tab
    xdg-open http://10.192.0.2:31112

    # Comment out for now as Grafana Dashboard is probably more user-friendly.
    # Open OpenFaaS Prometheus metrics UI in new tab
#    xdg-open http://10.192.0.2:31119

    GRAFANA_PORT=$(kubectl -n openfaas get svc grafana -o jsonpath="{.spec.ports[0].nodePort}")
    GRAFANA_URL=http://10.192.0.2:$GRAFANA_PORT/dashboard/db/openfaas

    echo "GRAFANA_URL=$GRAFANA_URL"

    # Open OpenFaaS Grafana metrics UI in new tab
    xdg-open $GRAFANA_URL 
}

# Deploy OpenFaaS Demo Functions, taken from:
# https://github.com/openfaas/workshop/blob/kubernetes-preview/lab2.md
# faas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml
function cluster::deploy-openfaas-demo-functions {
    OPENFAAS_URL=http://10.192.0.2:31112 faas-cli deploy -f ./openfaas-demo-functions.yaml
}


# Provision Fn
#-------------------------------------------------------------------------------

# Deploy Fn
# In an ideal world the Helm chart described in the fn-helm link below would
# "just work" however there are a few issues that caused problems. Firstly
# the chart only supports IngressController and LoadBalancer but I wanted
# to use NodePort initially so I can use Fn and OpenFaaS in the same way.
# Secondly *.svc.cluster.local addresses seem to cause issues. This is DNS
# related and I *think* that the issue is because I'm using mDNS/AVAHI so that
# hosts on my local private network can be resolved using <hostname>.local
# addresses, but I think that the default ndots:0 in /etc/resolv.conf means that
# *.svc.cluster.local are being looked up and failing to be resolved. Using
# *.svc.cluster.local. (with a trailing period) specify a fully qualified name
# which fixes the issue but tweaking kebeadm-dind-cluster DNS config may be
# a better approach when I can figure out how to do that.
function cluster::deploy-fn {
    # https://dzone.com/articles/serverless-with-fn-project-on-kubernetes-for-docke
    # https://github.com/fnproject/fn-helm
    # https://medium.com/@brianbmathews/going-serverless-on-oracle-cloud-using-the-open-source-fn-project-3c71f843b6d

    # TODO do this via YAML?
    kubectl create namespace fn

    git clone https://github.com/fnproject/fn-helm.git

    helm dep build fn-helm/fn


    # These two calls seem to be equivalent.
#    helm install --name fn --namespace fn fn-helm/fn
#    helm upgrade fn --install fn-helm/fn --namespace fn


    # Generate YAML from Fn Helm chart. We need to edit the YAML as the
    # available Helm configuration doesn't support our needs.
#    helm template --name fn --namespace fn fn-helm/fn > fn.yaml
    kubectl apply -f ./fn.yaml


    # To undeploy Fn from cluster.
    # helm delete --purge fn
    # kubectl delete namespace fn   
}

function cluster::up {
    # -------------- Infrastructure --------------
    cluster::deploy-kubeadm-dind-cluster
    cluster::deploy-helm

    # ----------------- OpenFaaS -----------------
    #cluster::deploy-openfaas
    cluster::deploy-openfaas-via-yaml # Plan B
    cluster::deploy-openfaas-grafana

    cluster::wait-for-openfaas-services

    cluster::deploy-openfaas-demo-functions
    cluster::run-openfaas-ui

    # -------------------- Fn --------------------
    cluster::deploy-fn
}


#-------------------------------------------------------------------------------

case ${COMMAND} in
  up)
    cluster::up
    ;;
  down)
    echo "Stopping cluster"
    ./dind-cluster-v1.13.sh down
    ;;
  clean)
    echo "Removing cluster containers and volumes"
    ./dind-cluster-v1.13.sh clean
    ;;
  *)
    echo "usage:" >&2
    echo "  $0 up - startup the cluster and provision services" >&2
#    echo "  $0 reup" >&2
    echo "  $0 down - stop and tear down the cluster" >&2
#    echo "  $0 init kubeadm-args..." >&2
#    echo "  $0 join kubeadm-args..." >&2
    # echo "  $0 bare container_name [docker_options...]"
    echo "  $0 clean - remove cluster containers and volumes"
#    echo "  $0 copy-image [image_name]" >&2
#    echo "  $0 e2e [test-name-substring]" >&2
#    echo "  $0 e2e-serial [test-name-substring]" >&2
#    echo "  $0 dump" >&2
#    echo "  $0 dump64" >&2
#    echo "  $0 split-dump" >&2
#    echo "  $0 split-dump64" >&2
    exit 1
    ;;
esac

